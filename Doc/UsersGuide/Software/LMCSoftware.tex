\section{Code structure}

The code structure in the {\tt LMC/} directory is as follows:
\begin{itemize}
\item {\tt bin/}: various problem implementations, including:
  \begin{itemize}
  \item {\tt FlameInABox/}: run directory for the flame-in-a-box
  \end{itemize}

\item {\tt src/}: source code

\item {\tt src\_sdc/}: alternative source code, conditionally compiled if {\tt USE\_LMC\_SDC=TRUE}

\item {\tt doc/}: documentation, including:
\begin{itemize}
 \item {\tt UsersGuide/}: you're reading this now!
\end{itemize}

\item {\tt tools/}: a catch-all for additional things you may need 
\end{itemize}


\section{An Overview of \lmc}

\lmc\ is built upon the \boxlib\ C++ framework.  This provides
high-level classes for managing an adaptive mesh refinement simulation,
including the core data structures required in AMR calculations.

The \lmc\ simulation begins in {\tt main.cpp} where an instance
of the \boxlib\ {\tt Amr} class is created:
\begin{lstlisting}
  Amr* amrptr = new Amr;
\end{lstlisting}
The initialization, including calling a problem's {\tt initdata()}
routine and refining the base grid occurs next through
\begin{lstlisting}
  amrptr->init(strt_time,stop_time);
\end{lstlisting}
And then comes the main loop over coarse timesteps until the
desired simulation time is reached:
\begin{lstlisting}
  while ( amrptr->okToContinue()                            &&
         (amrptr->levelSteps(0) < max_step || max_step < 0) &&
         (amrptr->cumTime() < stop_time || stop_time < 0.0) )

  {
     //
     // Do a timestep.
     //
     amrptr->coarseTimeStep(stop_time);
  }
\end{lstlisting}
This uses the \boxlib\ machinery to do the necessary subcycling in time,
including synchronization between levels, to advance the level hierarchy
forward in time.  

\subsection{Geometry class}

\subsection{ParmParse class}

\subsection{LMC Data Structures}

\subsubsection{State Data}

The {\tt StateData} class structure defined by \boxlib\ is the data container
used to store the field data associated with the state on a single AMR level
during an \lmc\ run.  The entire state consists of a dynamic union, or hierarchy, of
nested {\tt StateData} objects.  Periodic regrid operations modify the hierarchy,
changing the shape of the data containers at the various levels according to
user-specified criteria; new {\tt StateData} objects are created
for the affected levels, and are filled with the ``best'' (finest) available 
data at each location. Instructions for building and managing {\tt StateData} are
encapsulated in the \boxlib\ class, {\tt StateDescriptor}; as discussed later,
a {\tt StateDescriptor} will be created for each type of state field, and 
will include information about data centering, required grow cells, and
instructions for transferring data between AMR levels during various synchronization
operations.

In {\tt HeatTransfer.H}, the {\tt enum} {\tt StateType} defines the
different state descriptors for \lmc.  These are setup during the
run by code in {\tt HT\_setup.cpp}, and include:
\begin{itemize}
\item {\tt State\_Type}: the cell-centered thermodynamic state variables.

\item {\tt Press\_Type}: the node-centered dynamic pressure field.

\item {\tt Ydot\_Type}: the mass production rates for the chemical species.  
  
\end{itemize}

Each {\tt StateData} object has two {\tt MultiFabs}, one each for 
old and new times, and can provide an interpolated copy of the state at any time between the two.
Alternatively, can also access the data containers directly, for instance:
\begin{lstlisting}
MultiFab& S_new = get_new_data(State_Type);
\end{lstlisting}
gets a pointer to the multifab containing the hydrodynamics state data
at the new time (here {\tt State\_Type} is the {\tt enum} defined in 
{\tt HeatTransfer.H}).

{\tt MultiFab} data is distributed in space at the granularity of 
each {\tt Box} in its {\tt BoxArray}.  We iterate over {\it MultiFab}s using a special
iterator, {\tt MFIter}, which knows about the locality of the data---only the boxes owned by the
processor will be included in the loop on each processor.  An example loop (for the
initialization, taken from code in {\tt HT\_setup.cpp}):
\begin{lstlisting}
for (MFIter mfi(S_new); mfi.isValid(); ++mfi)
  {
     const Box& bx      = mfi.validbox();
     const int* lo      = bx.loVect();
     const int* hi      = bx.hiVect();

     if (! orig_domain.contains(bx)) {
        BL_FORT_PROC_CALL(CA_INITDATA,ca_initdata)
          (level, cur_time, lo, hi, ns,
           BL_TO_FORTRAN(S_new[mfi]), dx,
           gridloc.lo(), gridloc.hi());
     }
  }
\end{lstlisting}
Here {\tt BL\_TO\_FORTRAN} is a special \boxlib\ macro that converts the
C++ multifab into a Fortran array, and {\tt BL\_FORT\_PROC\_CALL}
is a BoxLib macro that is used to interface with Fortran routines. 
{\tt ++mfi} iterates to the next FArrayBox owned by the MultiFab, 
and {\tt mfi.isValid()} returns {\tt false} after we've reached 
the last box contained in the MultiFab, terminating the loop.

The corresponding Fortran function will look like:
{\color{red} Need to write the Fortran version here}


\subsection{Derived Variables}

\subsection{Error Estimators}


\subsection{Gravity class}


\subsection{Fortran Helper Modules}


\section{Setting Up Your Own Problem}

To define a new problem, we create a new directory under {\tt bin/},
and place in it a {\tt Prob\_2d.f90} file (or {\tt Prob\_3d.f90},
depending on the dimensionality of the problem), a {\tt probdata.f90}
file, the {\tt inputs} and {\tt probin} files, and a {\tt
  Make.package} file that tells the build system what problem-specific
routines exist.  The
simplest way to get started is to copy these files from an existing
problem.  Here we describe how to customize your problem.

The purpose of these files is:
\begin{itemize}
\item {\tt probdata.f90}: this holds the {\tt probdata\_module} Fortran module
  that allocates storage for all the problem-specific runtime parameters that
  are used by the problem (including those that are read from the {\tt probin}
  file.

\item {\tt Prob\_?d.f90}: this holds the main routines to
  initialize the problem and grid and perform problem-specific boundary
  conditions:

  \begin{itemize}
  \item {\tt probinit()}:

    This routine is primarily responsible for reading in the {\tt
      probin} file (by defining the {\tt \&fortin} namelist and
    reading in an initial model (usually through the {\tt
      model\_parser\_module}---see the {\tt toy\_convect} problem
    setup for an example).  The parameters that are initialized
    here are those stored in the {\tt probdata\_module}.

  \item {\tt initdata()}:

    This routine will initialize the state data for a single grid.
    The inputs to this routine are:
    \begin{itemize}
    \item {\tt level}: the level of refinement of the grid we are filling

    \item {\tt time}: the simulation time

    \item {\tt lo()}, {\tt hi()}: the integer indices of the box's {\em
      valid data region} lower left and upper right corners.  These
      integers refer to a global index space for the level and
      identify where in the computational domain the box lives.

    \item {\tt nscal}: the number of scalar quantities---this is not typically
      used in \lmc.

    \item {\tt state\_l1}, {\tt state\_l2}, ({\tt state\_l3}): the
      integer indices of the lower left corner of the box in each
      coordinate direction.  These are for the box as allocated in memory,
      so they include any ghost cells as well as the valid data regions.

    \item {\tt state\_h1}, {\tt state\_h2}, ({\tt state\_h3}): the
      integer indices of the upper right corner of the box in each
      coordinate direction.  These are for the box as allocated in memory,
      so they include any ghost cells as well as the valid data regions.
      
    \item {\tt state()}: the main state array.  This is dimensioned as:
\begin{verbatim}
double precision state(state_l1:state_h1,state_l2:state_h2,NVAR)
\end{verbatim}
    (in 2-d), where {\tt NVAR} comes from the {\tt meth\_params\_module}.

    When accessing this array, we use the index keys provided by
    {\tt meth\_params\_module} (e.g., {\tt Density}) to refer to specific
    quantities
    
    \item {\tt delta()}: this is an array containing the zone width ($\Delta x$)
      in each coordinate direction: $\mathtt{delta(1)} = \Delta x$,
      $\mathtt{delta(2)} = \Delta y$, $\ldots$.

    \item {\tt xlo()}, {\tt xhi()}: these are the physical coordinates of the
      lower left and upper right corners of the {\em valid region}
      of the box.  These can be used to compute the coordinates of the
      cell-centers of a zone as:
\begin{lstlisting}
  do j = lo(2), hi(2)
     y = xlo(2) + delta(2)*(dble(j-lo(2)) + 0.5d0)
     ...
\end{lstlisting}
     (Note: this method works fine for the problem initialization
     stuff, but for routines that implement tiling, as discussed below,
     {\tt lo} and {\tt xlo} may not refer to the same corner, and instead
     coordinates should be computed using {\tt problo()} from the {\tt
     prob\_params\_module}.)
     
    \end{itemize}
  \end{itemize}    

\item {\tt Prob\_?d.f90}:

  These routines handle how \lmc\ fills ghostcells {\em
  at physical boundaries} for specific data.
  These routines are registered in {\tt HT\_setup.cpp}, and
  called as needed.  By default, they just
  pass the arguments through to {\tt filcc}, which handles all of
  the generic boundary conditions (like reflecting, extrapolation,
  etc.).  The specific `{\tt fill}' routines can then supply the
  problem-specific boundary conditions, which are typically just
  Dirichlet boundary conditions (usually this means looking to see
  if the {\tt bc()} flag at a boundary is {\tt EXT\_DIR}.  The
  problem-specific code implementing these specific conditions
  should {\em follow} the {\tt filcc} call.
        
  \begin{itemize}
  \item {\tt velfill}:
    This handles the boundary filling for velocity fields.

  \item {\tt denfill}: 
    This handles the boundary filling for density field.

  \item {\tt rhohfill}: 
    This handles the boundary filling for $\rho h$ field.

  \item {\tt tempfill}: 
    This handles the boundary filling for temperature field.

  \item {\tt chemfill}: This handles boundary filling for
    $\rho Y_i$, for $i\in(1,n)$ and $n$ is the number of chemical species.

  \end{itemize}

  These routines take the following arguments:
  \begin{itemize}
  \item {\tt adv\_l1}, {\tt adv\_l2}, ({\tt adv\_l3}): the indicies of
    the lower left corner of the box holding the data we are working on.
    These indices refer to the entire box, including ghost cells.

  \item {\tt adv\_h1}, {\tt adv\_h2}, ({\tt adv\_h3}): the indicies of
    the upper right corner of the box holding the data we are working on.
    These indices refer to the entire box, including ghost cells.

  \item {\tt adv()}: the array of data whose ghost cells we are filling.
    Depending on the routine, this may have an additional index refering
    to the variable.
    
    This is dimensioned as:
\begin{verbatim}
  double precision adv(adv_l1:adv_h1,adv_l2:adv_h2)                             
\end{verbatim}

  \item {\tt domlo()}, {\tt domhi()}: the integer indices of the lower
    left and upper right corners of the valid region of the {\em entire
    domain}.  These are used to test against to see if we are filling
    physical boundary ghost cells.

    This changes according to refinement level: level-0 will
    range from {\tt 0} to {\tt amr.max\_grid\_size},
    and level-n will range from {\tt 0} to
    $\mathtt{amr.max\_grid\_size} \cdot \prod_n \mathtt{amr.ref\_ratio(n)}$.

  \item {\tt delta()}: is the zone width in each coordinate direction,
    as in {\tt initdata()} above.

  \item {\tt xlo()}: this is the physical coordinate of the lower
    left corner of the box we are filling---including the ghost cells.

    Note: this is different than how {\tt xlo()} was defined in
    {\tt initdata()} above.

  \item {\tt time}: the simulation time

  \item {\tt bc()}: an array that holds the type of boundary conditions
    to enforce at the physical boundaries for {\tt adv}.

    Sometimes it appears of the form {\tt bc(:,:)} and sometimes
    {\tt bc(:,:,:)}---the last index of the latter holds the variable
    index, i.e., density, pressure, species, etc.

    The first index is the coordinate direction and the second index
    is the domain face ({\tt 1} is low, {\tt 2} is hi), so {\tt
    bc(1,1)} is the lower $x$ boundary type, {\tt bc(1,2)} is
    the upper $x$ boundary type, {\tt bc(2,1)} is the lower
    $y$ boundary type, etc.

    To interpret the array values, we test against the quantities
    defined in {\tt bc\_types.fi} included in each subroutine,
    for example, {\tt EXT\_DIR}, {\tt FOEXTRAP}, $\ldots$.  The
    meaning of these are explained below.
      
  \end{itemize}
    
\end{itemize}


\section{Boundaries}
In \boxlib, we are primarily concerned with enabling structured-grid
computations.  A key aspect of this is the use of ``grow'' cells
around the ``valid box'' of cells over which we wish to apply stencil operations.
Grow cells, filled properly, are conveniently located temporary 
data containers that allow us to separate the steps of data preparation
(including communication, interpolation, or other complex manipulation)
from stencil application.  The steps that are required to fill grow cells
depends on where the cells ``live'' in the computational domain.

\subsection{Boundaries Between Grids and Levels}
Most of our state data is cell-centered, and often the grow cells are
as well.  When the cells lie directly over cells of a neighboring box
at the same AMR refinement level, these are ``fine-fine'' cells, and are
filled by direct copy (including any MPI communication necessary to enable
that copy).  Note that fine-fine boundary also include grow cells that
cover valid fine cells through a periodic boundary.

When the boundary between valid and grow cells is coincident
with a coarse-fine boundary, these coarse-fine grow cells will hold cell-centered 
temporary data that generated by interpolation (in space and time) of the
underlying coarse data.  This operation requires auxiliary metadata to define 
how the interpolation is to be done, in both space and time.  Importantly,
the interpolation also requires that coarse data be well-defined over
a time interval that brackets the time instant for which we are evaluating
the grow cell value  -- this places requirements on how the time-integration 
of the various AMR levels are sequenced relative to eachother.
In \boxlib, the field data associated with the system state, as well as the metadata
associated with inter-level transfers, is bundled (encapsulated) in
a class called ``StateData''.  The metadata 
is defined in {\tt HT\_setup.cpp} -- search for
{\tt cell\_cons\_interp}, for example -- which is ``cell conservative
interpolation'', i.e., the data is cell-based (as opposed to node-based
or edge-based) and the interpolation is such that the average of the
fine values created is equal to the coarse value from which they came.
(This wouldn't be the case with straight linear interpolation, for
example.)  A number of interpolators are provided with \boxlib\ and 
user-customizable ones can be added on the fly.

\subsection{Physical Boundaries}
The last type of grow cell exists at physical boundaries.  These are special for 
a couple of reasons.  First, the user must explicitly specify how they are to be
filled, consistent with the problem being run.  \boxlib\ provides a number of 
standard condition types typical of PDE problems (reflecting, extrapolated, etc),
and a special one that indicates external Dirichlet. In the case of Dirichlet,
the user will supply a coded function to fill grow cells (discussed elsewhere in
this document).

It is important to note that Dirichlet boundary data is to be specified as 
if applied on the edge of the cell bounding the domain. The array passed into the 
user boundary condition code is filled with cell-centered values in the valid 
region and in fine-fine, and coarse-fine grow cells.  Additionally, grow cells 
for standard extrapolation and reflecting boundaries are pre-filled.  The 
differential operators throughout \lmc\ are aware of the special boundaries
that are Dirichlet and wall-centered, and the stencils are adjusted accordingly.

For convenience, \lmc\ provides a limited set of mappings from a physics-based boundary condition
specification to a mathematical one that the code can apply.  This set can be extended
by adjusting the corresponding translations in {\tt HT\_setup.cpp}, but, by default, includes 
(See {\tt BoxLib/Src/C\_AMRLib/amrlib/BC\_TYPES.H} for more detail):
\begin{itemize}
\item {\it Outflow}:
  \begin{itemize}
    \item velocity: {\tt FOEXTRAP}
    \item temperature: {\tt FOEXTRAP}
    \item scalars: {\tt FOEXTRAP}
  \end{itemize}
  
\item {\it No Slip Wall with Adiabatic Temp}:
  \begin{itemize}
  \item velocity: {\tt EXT\_DIR}, $u=v=0$
  \item temperature: {\tt REFLECT\_EVEN}, $dT/dt=0$
  \item scalars: {\tt HOEXTRAP}
  \end{itemize}

\item {\it No Slip Wall with Fixed Temp}:
  \begin{itemize}
  \item velocity: {\tt EXT\_DIR}, $u=v=0$
  \item temperature: {\tt EXT\_DIR}
  \item scalars: {\tt HOEXTRAP}
  \end{itemize}
    
\item {\it Slip Wall with Adiabatic Temp}:
  \begin{itemize}
  \item velocity: {\tt EXT\_DIR}, $u_n=0$; {\tt HOEXTRAP}, $u_t$
  \item temperature: {\tt REFLECT\_EVEN}, $dT/dn=0$
  \item scalars: {\tt HOEXTRAP}
  \end{itemize}
  
\item {\it Slip Wall with Fixed Temp}:
  \begin{itemize}
  \item velocity: {\tt EXT\_DIR}, $u_n=0$
  \item temperature: {\tt EXT\_DIR}
  \item scalars: {\tt HOEXTRAP}
  \end{itemize}

\end{itemize}

The keywords used above are defined:
\begin{itemize}
\item {\tt INT\_DIR}: data taken from other grids or interpolated

\item {\tt EXT\_DIR}: data specified on EDGE (FACE) of bndry

\item {\tt HOEXTRAP}: higher order extrapolation to EDGE of bndry

\item {\tt FOEXTRAP}: first order extrapolation from last cell in interior

\item {\tt REFLECT\_EVEN}: $F(-n) = F(n)$ true reflection from interior cells

\item {\tt REFLECT\_ODD}: $F(-n) = -F(n)$ true reflection from interior cells
\end{itemize}


\subsection{The {\tt FillPatchIterator}}

A {\tt FillPatchIterator} is a \boxlib\ object tasked with the job of
filling rectangular patches of state data, possibly including grow cells,
and, if so, utilizing all the metadata  discussed above that is provided by
the user.  Thus, a {\tt FillPatchIterator} can only be constructed on
a fully registered {\tt StateData} object, and is the preferred 
process for filling grown platters of data prior to most stencil 
operations (e.g., explicit advection operators, which may require 
several grow cells).  It should be mentioned that a {\tt FillPatchIterator}
fills temporary data via copy operations, and therefore does not
directly modify the underlying state data.  In the code, if the state
is modified (e.g., via an advective ``time advance'', the new data
must be copied explicitly back into the {\tt StateData} containers.

For example, the following code demonstrates the calling sequence to
create and use a {\tt FillPatchIterator} for preparing a rectangular patch of 
data that includes the ``valid region'' plus {\tt NUM\_GROW} grow cells.  Here,
the valid region is specified as a union of rectangular boxes making up the 
box array underlying the {\tt MultiFab S\_new}, and {\tt NUM\_GROW} cells are 
added to each box in all directions to create the temporary patches to
be filled.  This is a parallel loop (the constructor is blocking over all
processors while the data is filled); each processor then gets platters of data
associated with the boxes from {\tt S\_new} that are local to it.


\begin{lstlisting}
   for (FillPatchIterator fpi(*this, S_new, NUM_GROW,
                              time, State_Type, strtComp, NUM_STATE);
         fpi.isValid(); ++fpi)
   {
     // Get a reference to the temporary platter of grown data
     FArrayBox &state = fpi();

     // work on "state"
     ...
   }           
\end{lstlisting}
Here the {\tt FillPatchIterator} fills the patch 
with data of type ``{\tt State\_Type}'' at time ``{\tt time}'',
starting with component {\tt strtComp} and including a total of
{\tt NUM\_STATE} components.  {\tt state} is a completely local
data structure, and will be processed serially by the owning 
procesor.  When the loop is terminated, the {\tt FillPatchIterator} 
and temporary data platters are destroyed (though much of the 
metadata generated during the operation is cached internally
for performance).  Notice that since {\tt NUM\_GROW} can be any
positive integer (i.e., that the grow region can extend over an arbitrary 
number of successively coarser AMR levels), this key operation can hide an
enormous amount of code and algorithm complexity.

\section{Parallel I/O}

Both checkpoint files and plotfiles are actually folders containing
subfolders: one subfolder for each level of the AMR hierarchy.
The fundamental data structure we read/write to disk is a MultiFab,
which is made up of multiple FAB's, one FAB per grid.  Multiple
MultiFabs may be written to each folder in a checkpoint file.
MultiFabs of course are shared across CPUs; a single MultiFab may be
shared across thousands of CPUs.  Each CPU writes the part of the
MultiFab that it owns to disk, but they don't each write to their own
distinct file.  Instead each MultiFab is written to a runtime
configurable number of files N (N can be set in the inputs file as the
parameter {\tt amr.checkpoint\_nfiles} and {\tt amr.plot\_nfiles}; the
default is 64).  That is to say, each MultiFab is written to disk
across at most N files, plus a small amount of data that gets written
to a header file describing how the file is laid out in those N files.

What happens is $N$ CPUs each opens a unique one of the $N$ files into
which the MultiFab is being written, seeks to the end, and writes
their data.  The other CPUs are waiting at a barrier for those $N$
writing CPUs to finish.  This repeats for another $N$ CPUs until all the
data in the MultiFab is written to disk.  All CPUs then pass some data
to CPU {\tt 0} which writes a header file describing how the MultiFab is
laid out on disk.

We also read MultiFabs from disk in a ``chunky'' manner, opening only $N$
files for reading at a time.  The number $N$, when the MultiFabs were
written, does not have to match the number $N$ when the MultiFabs are
being read from disk.  Nor does the number of CPUs running while
reading in the MultiFab need to match the number of CPUs running when
the MultiFab was written to disk.

Think of the number $N$ as the number of independent I/O pathways in
your underlying parallel filesystem.  Of course a ``real'' parallel
filesytem should be able to handle any reasonable value of $N$.  The
value {\tt -1} forces $N$ to the number of CPUs on which you're
running, which means that each CPU writes to a unique file, which can
create a very large number of files, which can lead to inode issues.


